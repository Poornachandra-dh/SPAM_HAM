{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f04b49",
   "metadata": {},
   "source": [
    "# SpamShield AI - Model Training Notebook\n",
    "\n",
    "This notebook walks through the complete process of training the spam detection model, from data loading to model evaluation and deployment.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Data Loading & Exploration](#data)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Feature Engineering](#features)\n",
    "5. [Model Training](#training)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Model Persistence](#persistence)\n",
    "8. [Testing Predictions](#testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d50378",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, precision_recall_fscore_support)\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b994119",
   "metadata": {},
   "source": [
    "### Download NLTK Data\n",
    "\n",
    "We need to download stopwords and lemmatization data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"âœ… NLTK data downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa3e08",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration\n",
    "\n",
    "We'll load three datasets:\n",
    "- `emails.csv` - Email spam dataset\n",
    "- `sms.csv` - SMS spam dataset\n",
    "- `spamham.csv` - Combined spam/ham messages\n",
    "\n",
    "All datasets will be merged into a single training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Load and combine all datasets\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # Load datasets\n",
    "    df_emails = pd.read_csv('emails.csv')\n",
    "    df_sms = pd.read_csv('sms.csv', encoding='latin-1')\n",
    "    df_spamham = pd.read_csv('spamham.csv')\n",
    "    \n",
    "    print(f\"Emails dataset: {df_emails.shape}\")\n",
    "    print(f\"SMS dataset: {df_sms.shape}\")\n",
    "    print(f\"SpamHam dataset: {df_spamham.shape}\")\n",
    "    \n",
    "    return df_emails, df_sms, df_spamham\n",
    "\n",
    "df_emails, df_sms, df_spamham = load_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729190f6",
   "metadata": {},
   "source": [
    "### Explore Dataset Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check emails.csv structure\n",
    "print(\"=\" * 60)\n",
    "print(\"EMAILS.CSV STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_emails.head())\n",
    "print(f\"\\nColumns: {df_emails.columns.tolist()}\")\n",
    "print(f\"Shape: {df_emails.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df_emails.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6edc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sms.csv structure\n",
    "print(\"=\" * 60)\n",
    "print(\"SMS.CSV STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_sms.head())\n",
    "print(f\"\\nColumns: {df_sms.columns.tolist()}\")\n",
    "print(f\"Shape: {df_sms.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df_sms.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccadb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check spamham.csv structure\n",
    "print(\"=\" * 60)\n",
    "print(\"SPAMHAM.CSV STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_spamham.head())\n",
    "print(f\"\\nColumns: {df_spamham.columns.tolist()}\")\n",
    "print(f\"Shape: {df_spamham.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df_spamham.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14815494",
   "metadata": {},
   "source": [
    "### Merge Datasets\n",
    "\n",
    "Standardize column names and combine all datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc503d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(df_emails, df_sms, df_spamham):\n",
    "    \"\"\"Merge all datasets into a single DataFrame\"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    # Process emails.csv\n",
    "    if 'Message' in df_emails.columns and 'Label' in df_emails.columns:\n",
    "        dfs.append(df_emails[['Message', 'Label']])\n",
    "        print(f\"âœ… Added {len(df_emails)} samples from emails.csv\")\n",
    "    \n",
    "    # Process sms.csv\n",
    "    if 'Message' in df_sms.columns and 'Label' in df_sms.columns:\n",
    "        dfs.append(df_sms[['Message', 'Label']])\n",
    "        print(f\"âœ… Added {len(df_sms)} samples from sms.csv\")\n",
    "    \n",
    "    # Process spamham.csv\n",
    "    if 'Message' in df_spamham.columns and 'Label' in df_spamham.columns:\n",
    "        dfs.append(df_spamham[['Message', 'Label']])\n",
    "        print(f\"âœ… Added {len(df_spamham)} samples from spamham.csv\")\n",
    "    \n",
    "    # Combine\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Standardize labels\n",
    "    df['Label'] = df['Label'].astype(str).str.lower().map({\n",
    "        'spam': 1, \n",
    "        'ham': 0, \n",
    "        '1': 1, \n",
    "        '0': 0\n",
    "    })\n",
    "    \n",
    "    # Drop missing values\n",
    "    df.dropna(subset=['Message', 'Label'], inplace=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Total samples: {len(df)}\")\n",
    "    print(f\"ðŸ“Š Class distribution:\\n{df['Label'].value_counts()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = merge_datasets(df_emails, df_sms, df_spamham)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc83127",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24040f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['Label'].value_counts().plot(kind='bar', ax=axes[0], color=['#00e676', '#ff1744'])\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0=HAM, 1=SPAM)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['HAM', 'SPAM'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['Label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                 colors=['#00e676', '#ff1744'], labels=['HAM', 'SPAM'])\n",
    "axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Balance:\")\n",
    "print(f\"HAM: {(df['Label']==0).sum()} ({(df['Label']==0).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"SPAM: {(df['Label']==1).sum()} ({(df['Label']==1).sum()/len(df)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55071c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message length analysis\n",
    "df['message_length'] = df['Message'].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution by class\n",
    "df[df['Label']==0]['message_length'].hist(bins=50, alpha=0.7, label='HAM', \n",
    "                                           color='#00e676', ax=axes[0])\n",
    "df[df['Label']==1]['message_length'].hist(bins=50, alpha=0.7, label='SPAM', \n",
    "                                           color='#ff1744', ax=axes[0])\n",
    "axes[0].set_title('Message Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Message Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='message_length', by='Label', ax=axes[1])\n",
    "axes[1].set_title('Message Length by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class (0=HAM, 1=SPAM)')\n",
    "axes[1].set_ylabel('Message Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMessage Length Statistics:\")\n",
    "print(df.groupby('Label')['message_length'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621d700",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We'll apply the following preprocessing steps:\n",
    "1. Convert to lowercase\n",
    "2. Remove special characters and numbers\n",
    "3. Tokenize\n",
    "4. Remove stopwords\n",
    "5. Lemmatize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5bee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for ML model\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase\n",
    "    2. Remove special chars\n",
    "    3. Tokenize\n",
    "    4. Remove stopwords\n",
    "    5. Lemmatize\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"FREE WINNER! Click here to claim your $1000 prize NOW!!!\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Processed: {preprocess_text(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c33b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all messages\n",
    "print(\"Preprocessing messages...\")\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "\n",
    "df['clean_message'] = df['Message'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"\\nâœ… Preprocessing complete!\")\n",
    "print(f\"\\nSample processed messages:\")\n",
    "print(df[['Message', 'clean_message']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05753568",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We'll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text to numerical features.\n",
    "\n",
    "**Parameters:**\n",
    "- `max_features=5000`: Keep top 5000 most important features\n",
    "- `ngram_range=(1, 3)`: Use 1-grams, 2-grams, and 3-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fe990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df['clean_message']\n",
    "y = df['Label']\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Labels (y): {y.shape}\")\n",
    "print(f\"\\nLabel distribution:\\n{y.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest set distribution:\\n{y_test.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a6f50",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We'll train an **Ensemble Voting Classifier** combining:\n",
    "1. **Multinomial Naive Bayes** - Fast, works well with text\n",
    "2. **Logistic Regression** - Linear classifier\n",
    "3. **Random Forest** - Non-linear ensemble\n",
    "\n",
    "The pipeline includes:\n",
    "- **TF-IDF Vectorization**: Convert text to features\n",
    "- **SMOTE**: Balance classes by oversampling minority class\n",
    "- **Voting Classifier**: Combine predictions from all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60872e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual classifiers\n",
    "nb = MultinomialNB()\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('nb', nb),\n",
    "        ('lr', lr),\n",
    "        ('rf', rf)\n",
    "    ],\n",
    "    voting='soft'  # Use probability-based voting\n",
    ")\n",
    "\n",
    "print(\"âœ… Classifiers defined:\")\n",
    "print(f\"  1. Multinomial Naive Bayes\")\n",
    "print(f\"  2. Logistic Regression (max_iter=1000)\")\n",
    "print(f\"  3. Random Forest (n_estimators=100)\")\n",
    "print(f\"\\nâœ… Ensemble: Voting Classifier (soft voting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build complete pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 3))),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', voting_clf)\n",
    "])\n",
    "\n",
    "print(\"âœ… Pipeline created:\")\n",
    "print(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"ðŸš€ Training model... (this may take a few minutes)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"â±ï¸  Training time: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251777d5",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate our model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "print(\"âœ… Predictions complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fad8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=['HAM', 'SPAM']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3207239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', \n",
    "            xticklabels=['HAM', 'SPAM'], \n",
    "            yticklabels=['HAM', 'SPAM'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives (HAM predicted as HAM): {cm[0][0]}\")\n",
    "print(f\"False Positives (HAM predicted as SPAM): {cm[0][1]}\")\n",
    "print(f\"False Negatives (SPAM predicted as HAM): {cm[1][0]}\")\n",
    "print(f\"True Positives (SPAM predicted as SPAM): {cm[1][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e886d8",
   "metadata": {},
   "source": [
    "## 7. Model Persistence\n",
    "\n",
    "Save the trained model and metrics for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce39be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(pipeline, 'spam_detector_model.joblib')\n",
    "print(\"âœ… Model saved: spam_detector_model.joblib\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'confusion_matrix': cm.tolist()\n",
    "}\n",
    "joblib.dump(metrics, 'model_metrics.joblib')\n",
    "print(\"âœ… Metrics saved: model_metrics.joblib\")\n",
    "\n",
    "# Check file sizes\n",
    "import os\n",
    "model_size = os.path.getsize('spam_detector_model.joblib') / (1024 * 1024)\n",
    "print(f\"\\nModel file size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7596c5a",
   "metadata": {},
   "source": [
    "## 8. Testing Predictions\n",
    "\n",
    "Let's test our model with some sample messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c30904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spam(message, model=pipeline):\n",
    "    \"\"\"Predict if a message is spam or ham\"\"\"\n",
    "    # Preprocess\n",
    "    clean_msg = preprocess_text(message)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict([clean_msg])[0]\n",
    "    proba = model.predict_proba([clean_msg])[0]\n",
    "    \n",
    "    label = \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "    confidence = proba[1] if prediction == 1 else proba[0]\n",
    "    \n",
    "    return label, confidence * 100\n",
    "\n",
    "# Test function\n",
    "test_message = \"FREE WINNER! Claim your prize now!\"\n",
    "label, conf = predict_spam(test_message)\n",
    "print(f\"Message: {test_message}\")\n",
    "print(f\"Prediction: {label} ({conf:.2f}% confidence)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple examples\n",
    "test_messages = [\n",
    "    \"FREE WINNER! Click here to claim your $1000 prize NOW!!!\",\n",
    "    \"Congratulations! You've won a free iPhone. Call now!\",\n",
    "    \"URGENT: Your account will be closed. Click this link immediately!\",\n",
    "    \"Hey, are we still meeting for lunch tomorrow?\",\n",
    "    \"Can you send me the project report by Friday?\",\n",
    "    \"Thanks for your help with the presentation!\",\n",
    "    \"Meeting at 3pm in conference room B\",\n",
    "    \"Win big money fast! No risk! Click here!\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for msg in test_messages:\n",
    "    label, conf = predict_spam(msg)\n",
    "    emoji = \"ðŸš¨\" if label == \"SPAM\" else \"âœ…\"\n",
    "    print(f\"\\n{emoji} {label} ({conf:.2f}%)\")\n",
    "    print(f\"   Message: {msg[:60]}{'...' if len(msg) > 60 else ''}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa688712",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Architecture\n",
    "- **Pipeline**: TF-IDF â†’ SMOTE â†’ Voting Classifier\n",
    "- **Classifiers**: Naive Bayes + Logistic Regression + Random Forest\n",
    "- **Features**: 5000 TF-IDF features with 1-3 n-grams\n",
    "\n",
    "### Performance\n",
    "Model performance metrics will be displayed after training completion.\n",
    "\n",
    "### Deployment\n",
    "The model has been saved and is ready for deployment in the Flask application.\n",
    "\n",
    "Files created:\n",
    "- `spam_detector_model.joblib` - Trained model\n",
    "- `model_metrics.joblib` - Performance metrics\n",
    "\n",
    "### Next Steps\n",
    "1. Integrate model with Flask app (`app.py`)\n",
    "2. Test with real-world data\n",
    "3. Monitor performance and retrain as needed\n",
    "4. Consider advanced features (URL detection, sender analysis)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
